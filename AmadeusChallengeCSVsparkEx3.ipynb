{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.0.0.161:4040\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1565520790871)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-11 20:53:19 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import java.nio.file.Paths\r\n",
       "import org.apache.spark.sql._\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.functions._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@214d803\r\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.file.Paths\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "//now create session\n",
    "val spark: org.apache.spark.sql.SparkSession = SparkSession\n",
    "      .builder()\n",
    "      .appName(\"Amadeus challenge\")\n",
    "      .config(\"spark.master\", \"local\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root_path: String = ./\r\n",
       "searches_path: String = ./big_searches.csv\r\n",
       "df1: org.apache.spark.sql.DataFrame = [Date: string, Time: string ... 43 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val root_path: String = \"./\";//where to find files\n",
    "val searches_path: String = root_path + \"big_searches.csv\";\n",
    "val df1=spark.read.option(\"sep\", \"^\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(searches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- TxnCode: string (nullable = true)\n",
      " |-- OfficeID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- RoundTrip: string (nullable = true)\n",
      " |-- NbSegments: string (nullable = true)\n",
      " |-- Seg1Departure: string (nullable = true)\n",
      " |-- Seg1Arrival: string (nullable = true)\n",
      " |-- Seg1Date: string (nullable = true)\n",
      " |-- Seg1Carrier: string (nullable = true)\n",
      " |-- Seg1BookingCode: string (nullable = true)\n",
      " |-- Seg2Departure: string (nullable = true)\n",
      " |-- Seg2Arrival: string (nullable = true)\n",
      " |-- Seg2Date: string (nullable = true)\n",
      " |-- Seg2Carrier: string (nullable = true)\n",
      " |-- Seg2BookingCode: string (nullable = true)\n",
      " |-- Seg3Departure: string (nullable = true)\n",
      " |-- Seg3Arrival: string (nullable = true)\n",
      " |-- Seg3Date: string (nullable = true)\n",
      " |-- Seg3Carrier: string (nullable = true)\n",
      " |-- Seg3BookingCode: string (nullable = true)\n",
      " |-- Seg4Departure: string (nullable = true)\n",
      " |-- Seg4Arrival: string (nullable = true)\n",
      " |-- Seg4Date: string (nullable = true)\n",
      " |-- Seg4Carrier: string (nullable = true)\n",
      " |-- Seg4BookingCode: string (nullable = true)\n",
      " |-- Seg5Departure: string (nullable = true)\n",
      " |-- Seg5Arrival: string (nullable = true)\n",
      " |-- Seg5Date: string (nullable = true)\n",
      " |-- Seg5Carrier: string (nullable = true)\n",
      " |-- Seg5BookingCode: string (nullable = true)\n",
      " |-- Seg6Departure: string (nullable = true)\n",
      " |-- Seg6Arrival: string (nullable = true)\n",
      " |-- Seg6Date: string (nullable = true)\n",
      " |-- Seg6Carrier: string (nullable = true)\n",
      " |-- Seg6BookingCode: string (nullable = true)\n",
      " |-- From: string (nullable = true)\n",
      " |-- IsPublishedForNeg: string (nullable = true)\n",
      " |-- IsFromInternet: string (nullable = true)\n",
      " |-- IsFromVista: string (nullable = true)\n",
      " |-- TerminalID: string (nullable = true)\n",
      " |-- InternetOffice: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1_columns: Array[String] = Array(Date, Time, TxnCode, OfficeID, Country, Origin, Destination, RoundTrip, NbSegments, Seg1Departure, Seg1Arrival, Seg1Date, Seg1Carrier, Seg1BookingCode, Seg2Departure, Seg2Arrival, Seg2Date, Seg2Carrier, Seg2BookingCode, Seg3Departure, Seg3Arrival, Seg3Date, Seg3Carrier, Seg3BookingCode, Seg4Departure, Seg4Arrival, Seg4Date, Seg4Carrier, Seg4BookingCode, Seg5Departure, Seg5Arrival, Seg5Date, Seg5Carrier, Seg5BookingCode, Seg6Departure, Seg6Arrival, Seg6Date, Seg6Carrier, Seg6BookingCode, From, IsPublishedForNeg, IsFromInternet, IsFromVista, TerminalID, InternetOffice)\r\n",
       "df_searches: org.apache.spark.sql.DataFrame = [Date: string, Time: string ... 43 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1_columns = df1.columns.map(name=>name.trim)\n",
    "val df_searches = df1.toDF(df1_columns: _*)\n",
    "df_searches.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches: org.apache.spark.rdd.RDD[String] = ./big_searches.csv MapPartitionsRDD[10] at textFile at <console>:44\r\n",
       "searches_count: Long = 20008000\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val searches = sc.textFile(searches_path)//Load RDD 1\n",
    "val searches_count = searches.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLineSearches: (line: String)(String, String)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Lets define a function to take each line and return what we want\n",
    " * In particular, we want the column 12 with arr_port and 34 with pax\n",
    " * VIP: to handle the wrong lines reading (ex. 14 fields) we need to cover that exception, \n",
    " * otherwise it would corrupt the RDD and give many headaches...\n",
    " */\n",
    "def parseLineSearches(line:String) = {\n",
    "    val fields   = line.split(\"\\\\^\")\n",
    "    //check if the line is complete..\n",
    "    if (fields.length>34){\n",
    "    val date = fields(0).trim\n",
    "    val destination = fields(6).trim\n",
    "    \n",
    "    if (destination ==\" \" || date==\"\") (\"KKK\",\"0\") else\n",
    "    (destination, date)\n",
    "    }\n",
    "    else (\"KKK\",\"0\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches01: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[11] at map at <console>:48\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches01 = searches.map(parseLineSearches).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(String, String)] = Array((\"Destination\",\"Date\"), (\"AUH\",\"2013-01-01\"), (\"MIL\",\"2013-01-01\"), (\"SFO\",\"2013-01-01\"), (\"ARN\",\"2013-01-01\"), (\"MAD\",\"2013-01-01\"), (\"BLR\",\"2013-01-01\"), (\"PAR\",\"2013-01-01\"), (\"DUB\",\"2013-01-01\"), (\"ACE\",\"2013-01-01\"))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches01.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches02: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at filter at <console>:50\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches02=searches01.filter(e => e._1 == \"MAD\" || e._1 == \"BCN\" || e._1 == \"AGP\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches02: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[13] at filter at <console>:50\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches02=searches01.filter(record => List(\"MAD\",\"BCN\", \"AGP\").contains(record._1)).persist()//Check this Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
