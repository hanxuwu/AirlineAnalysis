{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root_path: String = ./\r\n",
       "searches_path: String = ./searches_sample.csv.bz2\r\n",
       "bookings_path: String = ./bookings_sample.csv.bz2\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val root_path: String = \"./\";//where to find files\n",
    "val searches_path: String = root_path + \"searches_sample.csv.bz2\";\n",
    "val bookings_path: String = root_path + \"bookings_sample.csv.bz2\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches: org.apache.spark.rdd.RDD[String] = ./searches_sample.csv.bz2 MapPartitionsRDD[1] at textFile at <console>:35\r\n",
       "bookings: org.apache.spark.rdd.RDD[String] = ./bookings_sample.csv.bz2 MapPartitionsRDD[3] at textFile at <console>:36\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches = sc.textFile(searches_path)//Load RDD 1\n",
    "val bookings = sc.textFile(bookings_path)//Load RDD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headerColumns_searches: List[String] = List(Date, Time, TxnCode, OfficeID, Country, Origin, Destination, RoundTrip, NbSegments, Seg1Departure, Seg1Arrival, Seg1Date, Seg1Carrier, Seg1BookingCode, Seg2Departure, Seg2Arrival, Seg2Date, Seg2Carrier, Seg2BookingCode, Seg3Departure, Seg3Arrival, Seg3Date, Seg3Carrier, Seg3BookingCode, Seg4Departure, Seg4Arrival, Seg4Date, Seg4Carrier, Seg4BookingCode, Seg5Departure, Seg5Arrival, Seg5Date, Seg5Carrier, Seg5BookingCode, Seg6Departure, Seg6Arrival, Seg6Date, Seg6Carrier, Seg6BookingCode, From, IsPublishedForNeg, IsFromInternet, IsFromVista, TerminalID, InternetOffice)\r\n",
       "head_searches: String = Date^Time^TxnCode^OfficeID^Country^Origin^Destination^RoundTrip^NbSegments^Seg1Departure^Seg1Arrival^Seg1Date^Seg1Carrier^Seg1BookingCode^Seg2Departure^Se..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val headerColumns_searches = searches.first().split(\"\\\\^\").to[List]\n",
    "val head_searches = sc.textFile(searches_path).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headerColumns_bookings: List[String] = List(\"act_date           \", source, pos_ctry, pos_iata, \"pos_oid  \", \"rloc          \", \"cre_date           \", duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_city, lst_ctry, brd_port, brd_city, brd_ctry, off_port, off_city, off_ctry, mkt_port, mkt_city, mkt_ctry, intl, \"route          \", carrier, bkg_class, cab_class, \"brd_time           \", \"off_time           \", pax, year, month, \"oid      \")\r\n",
       "head_bookings: String = \"act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^off_port^off_city^off_ctry^mkt_port^mkt_city^mkt_ctry^intl^route         ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val headerColumns_bookings = bookings.first().split(\"\\\\^\").to[List]\n",
    "val head_bookings = sc.textFile(bookings_path).first()\n",
    "head_bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches_count: Long = 10000\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches_count = searches.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bookings_count: Long = 10000\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bookings_count = bookings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Top 10 arrival airports in the world in 2013 (using the bookings file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLineBookings: (line: String)(String, String)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Lets define a function to take each line and return what we want\n",
    " * In particular, we want the column 12 with arr_port and 34 with pax\n",
    " * VIP: to handle the wrong lines reading (ex. 14 fields) we need to cover that exception, \n",
    " * otherwise it would corrupt the RDD and give many headaches...\n",
    " */\n",
    "def parseLineBookings(line:String) = {\n",
    "    val fields   = line.split(\"\\\\^\")\n",
    "    //check if the line is complete..\n",
    "    if (fields.length>34){\n",
    "    val arr_port = fields(12).trim\n",
    "    val paxstring = fields(34).trim\n",
    "    if (arr_port==\"\" || paxstring==\"\") (\"KKK\",\"0\") else\n",
    "    (arr_port, paxstring)\n",
    "    }\n",
    "    else (\"KKK\",\"0\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_rdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[16] at map at <console>:39\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Load the pair RDD from bookings with arr_port and pax\n",
    "val pair_rdd = bookings.map(parseLineBookings).persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[(String, String)] = Array((arr_port,pax), (LHR,-1), (CLT,1), (CLT,1), (SVO,1), (SVO,1), (LGA,1), (LGA,1), (SIN,2), (SIN,2))\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.take(10)//test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kk: Array[(String, String)] = Array()\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kk = pair_rdd.filter(_._1 != \"arr_port\" ).persist(MEMORY_AND_DISK).filter(_._1 == \"KKK\").collect()//testing filtersval kk = pair_rdd.filter(_._1 != \"arr_port\" ).persist(MEMORY_AND_DISK).filter(_._1 == \"KKK\").collect()//testing filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_rdd_good: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[20] at filter at <console>:40\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair_rdd_good = pair_rdd.filter(_._1 != \"arr_port\" ).filter(_._1 != \"KKK\").persist(MEMORY_AND_DISK)//now create filtered pairRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toInt: (s: String)Option[Int]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Define a conversion to integer with exception handling. \n",
    "//This is important to avoid NaN when trying to convert your strings to integers\n",
    "def toInt(s: String): Option[Int] = {\n",
    "  try {\n",
    "    Some(s.toInt)\n",
    "  } catch {\n",
    "    case e: Exception => None\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_bookings: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[21] at map at <console>:46\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//We map with some care using match to handle the exceptions. If we find a None we will write 0 pax.\n",
    "//Make sure wee have integers\n",
    "val pair_bookings = pair_rdd_good.map(x=> {\n",
    "      val tst = toInt(x._2)\n",
    "      tst  match {\n",
    "      case None => (x._1, 0)\n",
    "      case Some(y) => (x._1, y) }\n",
    "}).persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[(String, Int)] = Array((LHR,-1), (CLT,1), (CLT,1), (SVO,1), (SVO,1), (LGA,1), (LGA,1), (SIN,2), (SIN,2), (SIN,2))\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_bookings.take(10)//test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[22] at reduceByKey at <console>:47\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//reduceByKey will groupByKey and then do the required operation on the values!\n",
    "val temp = pair_bookings.reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[(String, Int)] = Array((BEG,1), (SKP,4), (AYT,-6), (THR,4), (LOS,2))\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.take(5)//test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top10: Array[(Int, String)] = Array((112,HKG), (95,LGA), (94,ORD), (92,JFK), (91,SFO), (91,LAX), (90,MCO), (82,DCA), (79,DEN), (76,LHR))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//In order to use the efficient transformation sortByKey we need to swap Keys and Values first.\n",
    "//val top10 = temp.map(x => (x._2,x._1)).sortByKey(false).take(10)\n",
    "val top10 = temp.map(_.swap).sortByKey(false).take(10)//swap does the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112,HKG)\n",
      "(95,LGA)\n",
      "(94,ORD)\n",
      "(92,JFK)\n",
      "(91,SFO)\n",
      "(91,LAX)\n",
      "(90,MCO)\n",
      "(82,DCA)\n",
      "(79,DEN)\n",
      "(76,LHR)\n"
     ]
    }
   ],
   "source": [
    "top10.foreach(e => println(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
