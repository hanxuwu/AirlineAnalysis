{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-11 21:36:24 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import java.nio.file.Paths\r\n",
       "import org.apache.spark.sql._\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.functions._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1c667d8a\r\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.file.Paths\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "//now create session\n",
    "val spark: org.apache.spark.sql.SparkSession = SparkSession\n",
    "      .builder()\n",
    "      .appName(\"Amadeus challenge\")\n",
    "      .config(\"spark.master\", \"local\")\n",
    "      .getOrCreate()\n",
    "\n",
    "//set new runtime options\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 2)\n",
    "spark.conf.set(\"spark.executor.memory\", \"3g\")\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel._\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root_path: String = ./\r\n",
       "searches_path: String = ./big_searches.csv\r\n",
       "bookings_path: String = ./big_bookings.csv\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val root_path: String = \"./\";//where to find files\n",
    "val searches_path: String = root_path + \"big_searches.csv\";\n",
    "val bookings_path: String = root_path + \"big_bookings.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches: org.apache.spark.rdd.RDD[String] = ./big_searches.csv MapPartitionsRDD[48] at textFile at <console>:83\r\n",
       "bookings: org.apache.spark.rdd.RDD[String] = ./big_bookings.csv MapPartitionsRDD[50] at textFile at <console>:84\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches = sc.textFile(searches_path) // load RDD1\n",
    "val bookings = sc.textFile(bookings_path) // load RDD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headerColumns_searches: List[String] = List(\"Date\", \"Time\", \"TxnCode\", \"OfficeID\", \"Country\", \"Origin\", \"Destination\", \"RoundTrip\", \"NbSegments\", \"Seg1Departure\", \"Seg1Arrival\", \"Seg1Date\", \"Seg1Carrier\", \"Seg1BookingCode\", \"Seg2Departure\", \"Seg2Arrival\", \"Seg2Date\", \"Seg2Carrier\", \"Seg2BookingCode\", \"Seg3Departure\", \"Seg3Arrival\", \"Seg3Date\", \"Seg3Carrier\", \"Seg3BookingCode\", \"Seg4Departure\", \"Seg4Arrival\", \"Seg4Date\", \"Seg4Carrier\", \"Seg4BookingCode\", \"Seg5Departure\", \"Seg5Arrival\", \"Seg5Date\", \"Seg5Carrier\", \"Seg5BookingCode\", \"Seg6Departure\", \"Seg6Arrival\", \"Seg6Date\", \"Seg6Carrier\", \"Seg6BookingCode\", \"From\", \"IsPublishedForNeg\", \"IsFromInternet\", \"IsFromVista\", \"TerminalID\", \"InternetOffice\")\r\n",
       "head_searches: String = \"Date\"^\"Time\"^\"TxnCode\"^\"OfficeID\"^\"Country\"^\"Origin\"^\"Destinati..."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val headerColumns_searches = searches.first().split(\"\\\\^\").to[List]\n",
    "val head_searches = sc.textFile(searches_path).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headerColumns_bookings: List[String] = List(\"act_date           \", \"source\", \"pos_ctry\", \"pos_iata\", \"pos_oid  \", \"rloc          \", \"cre_date           \", \"duration\", \"distance\", \"dep_port\", \"dep_city\", \"dep_ctry\", \"arr_port\", \"arr_city\", \"arr_ctry\", \"lst_port\", \"lst_city\", \"lst_ctry\", \"brd_port\", \"brd_city\", \"brd_ctry\", \"off_port\", \"off_city\", \"off_ctry\", \"mkt_port\", \"mkt_city\", \"mkt_ctry\", \"intl\", \"route          \", \"carrier\", \"bkg_class\", \"cab_class\", \"brd_time           \", \"off_time           \", \"pax\", \"year\", \"month\", \"oid      \")\r\n",
       "head_bookings: String = \"act_date           \"^\"source\"^\"pos_ctry\"^\"pos_iata\"^\"pos_oid  \"^\"rloc          \"^\"cre_date           \"^\"duration\"^\"distance\"^\"dep_port\"^\"dep_city\"^\"dep_ctry\"^\"arr_port\"^\"arr_city\"^\"arr_ctry\"^\"lst_port\"^\"lst_city\"^\"lst_ctry\"^\"brd_..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val headerColumns_bookings = bookings.first().split(\"\\\\^\").to[List]\n",
    "val head_bookings = sc.textFile(bookings_path).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: String = \"act_date           \"^\"source\"^\"pos_ctry\"^\"pos_iata\"^\"pos_oid  \"^\"rloc          \"^\"cre_date           \"^\"duration\"^\"distance\"^\"dep_port\"^\"dep_city\"^\"dep_ctry\"^\"arr_port\"^\"arr_city\"^\"arr_ctry\"^\"lst_port\"^\"lst_city\"^\"lst_ctry\"^\"brd_port\"^\"brd_city\"^\"brd_ctry\"^\"off_port\"^\"off_city\"^\"off_ctry\"^\"mkt_port\"^\"mkt_city\"^\"mkt_ctry\"^\"intl\"^\"route          \"^\"carrier\"^\"bkg_class\"^\"cab_class\"^\"brd_time           \"^\"off_time           \"^\"pax\"^\"year\"^\"month\"^\"oid      \"\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches_count: Long = 20008000\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches_count = searches.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bookings_count: Long = 10009000\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bookings_count = bookings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### Top 10 arrival airports in world in 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLineBookings: (line: String)(String, String)\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* Lets define a function to take each line and return what we want\n",
    " * In particular, we want the column 12 with arr_port and 34 with pax\n",
    " * VIP: to handle the wrong lines reading (ex. 14 fields) we need to cover that exception, \n",
    " * otherwise it would corrupt the RDD and give many headaches...\n",
    " */\n",
    "def parseLineBookings(line:String) = {\n",
    "    val fields   = line.split(\"\\\\^\")\n",
    "    //check if the line is complete..\n",
    "    if (fields.length>34){\n",
    "    val arr_port = fields(12).stripPrefix(\"\\\"\").stripSuffix(\"\\\"\").trim\n",
    "    val paxstring = fields(34).stripPrefix(\"\\\"\").stripSuffix(\"\\\"\").trim\n",
    "    if (arr_port==\"\" || paxstring==\"\") (\"KKK\",\"0\") else\n",
    "    (arr_port, paxstring)\n",
    "    }\n",
    "    else (\"KKK\",\"0\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_rdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[55] at map at <console>:87\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Load the pair RDD from bookings with arr_port and pax\n",
    "val pair_rdd = bookings.map(parseLineBookings).persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Array[(String, String)] = Array((arr_port,pax), (LHR,-1), (CLT,1), (CLT,1), (SVO,1), (SVO,1), (LGA,1), (LGA,1), (SIN,2), (SIN,2))\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.take(10)//test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kk: Array[(String, String)] = Array()\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kk = pair_rdd.filter(_._1 != \"arr_port\" ).persist().filter(_._1 == \"KKK\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_rdd_good: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[59] at filter at <console>:88\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair_rdd_good = pair_rdd.filter(_._1 != \"arr_port\" ).filter(_._1 != \"KKK\").persist()//now create filtered pairRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toInt: (s: String)Option[Int]\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Define a conversion to integer with exception handling. \n",
    "//This is important to avoid NaN when trying to convert your strings to integers\n",
    "def toInt(s: String): Option[Int] = {\n",
    "  try {\n",
    "    Some(s.toInt)\n",
    "  } catch {\n",
    "    case e: Exception => None\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_bookings: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[60] at map at <console>:94\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//We map with some care using match to handle the exceptions. If we find a None we will write 0 pax.\n",
    "//Make sure wee have integers\n",
    "val pair_bookings = pair_rdd_good.map(x=> {\n",
    "      val tst = toInt(x._2)\n",
    "      tst  match {\n",
    "      case None => (x._1, 0)\n",
    "      case Some(y) => (x._1, y) }\n",
    "}).persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Array[(String, Int)] = Array((LHR,-1), (CLT,1), (CLT,1), (SVO,1), (SVO,1), (LGA,1), (LGA,1), (SIN,2), (SIN,2), (SIN,2))\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_bookings.take(10)//test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Array[(String, Int)] = Array((LHR,-1), (CLT,1), (CLT,1), (SVO,1), (SVO,1), (LGA,1), (LGA,1), (SIN,2), (SIN,2), (SIN,2))\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_bookings.take(10)//test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[61] at reduceByKey at <console>:95\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//reduceByKey will groupByKey and then do the required operation on the values!\n",
    "val temp = pair_bookings.reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: Array[(String, Int)] = Array((DAY,2002), (HNL,34034), (BGI,10010), (IKT,0), (AJA,7007))\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.take(5)//test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top10: Array[(Int, String)] = Array((112112,HKG), (95095,LGA), (94094,ORD), (92092,JFK), (91091,LAX), (91091,SFO), (90090,MCO), (82082,DCA), (79079,DEN), (76076,LHR))\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//In order to use the efficient transformation sortByKey we need to swap Keys and Values first.\n",
    "//val top10 = temp.map(x => (x._2,x._1)).sortByKey(false).take(10)\n",
    "val top10 = temp.map(_.swap).sortByKey(false).take(10)//swap does the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112112,HKG)\n",
      "(95095,LGA)\n",
      "(94094,ORD)\n",
      "(92092,JFK)\n",
      "(91091,LAX)\n",
      "(91091,SFO)\n",
      "(90090,MCO)\n",
      "(82082,DCA)\n",
      "(79079,DEN)\n",
      "(76076,LHR)\n"
     ]
    }
   ],
   "source": [
    "top10.foreach(e => println(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
